---
title: "S2_code_supplement"
author: "Colin S. Beam"
date: "2025-02-12"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      out.extra = "", # This forces knitr to label all figures.
                      #fig.pos = 'H',  # hold the figure positions
                      #fig.pos='htbp', # put figure position approximately here
                      dev = "pdf"
                      )  

library(tidyverse)
library(kableExtra)
library(PRROC)
library(boot)
library(egg)
library(tinytex)
library(distill)
library(caret)

```


```{r plotting parameters}
# text size for graphs
axis_title_size <- 8
legend_title_size <- 8
axis_text_size <- 6
legend_text_size <- 6

# text size for relative resolution graphs
rr_axis_text_size <- 6
rr_axis_title_size <- 8

# two color palette
two_colors <- c("dodgerblue", "darkgoldenrod1")

```


## Overview

This technical supplement contains all of the code used for the resolving power paper. Associated data files can also be found in the same directory. For questions please contact [Colin Beam](colinbeam@gmail.com).

## 2. ROC and PR curves

```{r confusion_matrix}
confusion_matrix <- matrix(nrow = 3, ncol=2)
colnames(confusion_matrix) <- c("actual +", "actual -")
rownames(confusion_matrix) <- c("predicted +", "predicted -", "total")

confusion_matrix[1,] <- c("TP", "FP")
confusion_matrix[2,] <- c("FN", "TN")
confusion_matrix[3,] <- c("P", "N")

knitr::kable(confusion_matrix, caption = "Example confusion matrix", escape = FALSE, align = "c") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position"))

```


## 3. Mapping between metrics

```{r men versus women height}
# source: https://ourworldindata.org/human-height

# parameters
male_mean <- 178.4
male_sd <- 186 - male_mean

female_mean <- 164.7
female_sd <- 171.8 - female_mean

# find implied auroc
delta_c <- male_mean - female_mean
auroc <- pnorm(delta_c/sqrt(male_sd^2 + female_sd^2))

# alternate calculation: standardize by female distribution
#male_standard_mean <- (male_mean - female_mean)/female_sd
#male_standard_sd <- male_sd/female_sd
#pnorm(male_standard_mean/sqrt(1^2 + male_standard_sd^2))

# probabilistic interpretation
#nsim <- 10000
#men_heights <- rnorm(n = nsim, mean = male_mean, sd = male_sd)
#women_heights <- rnorm(n = nsim, mean = female_mean, sd = female_sd)
#mean(men_heights > women_heights)

# threshold
decision_threshold <- 171

```

```{r binormal model graphs}
# female distribution
female_data <- data.frame(cm = seq(female_mean - 3*female_sd, female_mean + 4*female_sd, length.out=1000))

female_data <- female_data %>% 
  mutate(y = dnorm(cm, mean = female_mean, sd = female_sd),
         distribution = "Women")

# male distribution
male_data <- data.frame(cm = seq(male_mean - 4*male_sd, male_mean + 3*male_sd, length.out=1000))

male_data <- male_data %>% 
  mutate(y = dnorm(cm, mean = male_mean, sd = male_sd),
         distribution = "Men")

plot_data <- bind_rows(female_data, male_data)

binormal_example <- ggplot(plot_data, aes(cm, y, color=distribution)) +
  geom_path() +
  geom_vline(xintercept = decision_threshold, linetype = "dashed") +
  #scale_linetype_manual(values = c("dashed", "solid")) +
  scale_color_manual(values = two_colors) +
  scale_x_continuous( breaks = c(164.7, 178.4) ) +
  theme_bw() +
  labs(
    x = "cm",
    y = "density",
    linetype="") +
  theme(
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size),
    legend.title = element_blank(),
    legend.text = element_text(size=legend_text_size)
    ) 

```

```{r binormal_example, fig.width=4, fig.height=1.5, fig.cap="\\label{fig:binormal_example}A binormal classifier example. The distribution of men's and women's heights approximately follow a normal distribution. The model implies an AUROC of .906. The vertical dashed line at 171 cm is an example decision threshold."}

binormal_example

```

## 4. Resolving power

```{r fictional signal functions}
#----
# auroc_to_ausgc
#
# For a given model AUROC and prevalence, give the AUSGC.
#
## arguments
# 
# auroc   model auroc
# v       prevalence of the positive class
#
auroc_to_ausgc <- function(auroc, v) {
  x <- (auroc - 0.5)/0.5  # map to 0-1 scale.
  beta0 <- log(v)
  beta1 <- -log(v)
  log_ausgc <-  beta0 + beta1*x
  ausgc <- exp(log_ausgc)
  ausgc
}


#----
# ausgc_to_auroc
#
# For a given model AUSGC and prevalence, give the AUSGC 
#
## arguments
# 
# ausgc   ausgc of the model
# v       prevalence of the model
ausgc_to_auroc <- function(ausgc, v) {
  beta0 <- log(v)
  beta1 <- -log(v)
  percentile <- ( log(ausgc) - beta0 )/(beta1)
  auroc <- 0.5*percentile + 0.5
  auroc
}



```


```{r metric comparison data}
#----
# Model compare data
prevalence <- 0.01
eps <- rep(c(.05, .05), each=2)

metric_label <- rep(c("AUROC", "AUSGC"), each=2)
model_label <- rep(c("A", "B"), times=2)

auroc_eval <- c(0.7, 0.9)
metric <- c(auroc_eval, auroc_to_ausgc(auroc_eval, v = prevalence))
metric_lower <- metric - eps
metric_upper <- metric + eps

auroc_lower <- c(metric_lower[1:2], ausgc_to_auroc(metric_lower[3:4], v = prevalence))

auroc_upper <- c(metric_upper[1:2], ausgc_to_auroc(metric_upper[3:4], v = prevalence))

perf_data <- data.frame(
  metric_label = metric_label,
  model=model_label,
  auroc_eval=rep(auroc_eval, times=2),
  metric_value=metric,
  metric_lower=metric_lower,
  metric_upper=metric_upper,
  auroc_lower=auroc_lower,
  auroc_upper=auroc_upper
) %>% 
  mutate(
    metric_label = factor(metric_label, levels = c("AUROC", "AUSGC"))
  )



```


```{r create data for method example}
grid_length <- 1000
point_size <- 2
bar_size <- .03

auroc_quality <- seq(0.5, 1, length.out=grid_length)
m1 <- auroc_quality
m2 <- auroc_to_ausgc(auroc_quality, v = prevalence)

metric_label <- rep(c("AUROC", "AUSGC"), each=grid_length)

metric_data <- data.frame(
  metric_label = metric_label,
  auroc = rep(auroc_quality, 2),
  metric_value = c(m1, m2)
  ) %>% 
  mutate(
    metric_label = factor(metric_label, levels = c("AUROC", "AUSGC"))
  )


```


```{r create AUROC 0.7 plot}
auroc_select <- 0.7

# perf data for only xth percentile
plot_data <- perf_data %>% 
  filter(auroc_eval==auroc_select) 

# ROC line segments
roc_lower_y <- plot_data$metric_lower[plot_data$metric_label=="AUROC"]
roc_lower_x2 <- roc_lower_y
roc_upper_y <- plot_data$metric_upper[plot_data$metric_label=="AUROC"]
roc_upper_x2 <- roc_upper_y


# SG line segments
sg_lower_y <- plot_data$metric_lower[plot_data$metric_label=="AUSGC"]
sg_lower_x2 <- ausgc_to_auroc(sg_lower_y, v = prevalence)
sg_upper_y <- plot_data$metric_upper[plot_data$metric_label=="AUSGC"]
sg_upper_x2 <- ausgc_to_auroc(sg_upper_y, v = prevalence)


# line segment data
segment_data <- data.frame(
  metric_label = c("AUROC", "AUSGC"),
  x1 = c(auroc_select, auroc_select),
  lower_y = c(roc_lower_y, sg_lower_y),
  lower_x2 = c(roc_lower_x2, sg_lower_x2),
  upper_y = c(roc_upper_y, sg_upper_y),
  upper_x2 = c(roc_upper_x2, sg_upper_x2)
) %>% 
  mutate(
    metric_label = factor(metric_label, levels=c("AUROC", "AUSGC"))
  )


####
# CI plot
ci_plot <- ggplot(metric_data, aes(auroc, metric_value)) +
  facet_wrap(~ metric_label, ncol=2) +
  geom_path(color="black") +
  xlab("Model quality (AUROC units)") +
  ylab("Metric value") +
  geom_point(mapping = aes(auroc_eval, metric_value), data = plot_data, size=point_size, color=two_colors[1]) +
  geom_errorbar(mapping = aes(auroc_eval, ymin=metric_lower, ymax=metric_upper), data = plot_data, width=bar_size, color=two_colors[1]) +
  geom_segment(aes(x = x1, y = lower_y, xend = lower_x2, yend = lower_y), data = segment_data, color=two_colors[1], linetype="dashed") +
  geom_segment(aes(x = lower_x2, y = lower_y, xend = lower_x2, yend = -Inf), data = segment_data, color=two_colors[1], linetype="dashed") +
  geom_segment(aes(x = x1, y = upper_y, xend = upper_x2, yend = upper_y), data = segment_data, color=two_colors[1], linetype="dashed") +
  geom_segment(aes(x = upper_x2, y = upper_y, xend = upper_x2, yend = -Inf), data = segment_data, color=two_colors[1], linetype="dashed") +
  theme_bw() +
  theme(
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size)
  ) +
  NULL

```


```{r create AUROC 0.9 plot}
auroc_select <- 0.9

# perf data for only xth percentile
plot_data <- perf_data %>% 
  filter(auroc_eval==auroc_select) 

# ROC line segments
roc_lower_y <- plot_data$metric_lower[plot_data$metric_label=="AUROC"]
roc_lower_x2 <- roc_lower_y
roc_upper_y <- plot_data$metric_upper[plot_data$metric_label=="AUROC"]
roc_upper_x2 <- roc_upper_y


# SG line segments
sg_lower_y <- plot_data$metric_lower[plot_data$metric_label=="AUSGC"]
sg_lower_x2 <- ausgc_to_auroc(sg_lower_y, v = prevalence)
sg_upper_y <- plot_data$metric_upper[plot_data$metric_label=="AUSGC"]
sg_upper_x2 <- ausgc_to_auroc(sg_upper_y, v = prevalence)


# line segment data
segment_data <- data.frame(
  metric_label = c("AUROC", "AUSGC"),
  x1 = c(auroc_select, auroc_select),
  lower_y = c(roc_lower_y, sg_lower_y),
  lower_x2 = c(roc_lower_x2, sg_lower_x2),
  upper_y = c(roc_upper_y, sg_upper_y),
  upper_x2 = c(roc_upper_x2, sg_upper_x2)
) %>% 
  mutate(
    metric_label = factor(metric_label, levels=c("AUROC", "AUSGC"))
  )



####
# CI plot

ci_plot <- ci_plot +
  geom_point(mapping = aes(auroc_eval, metric_value), data = plot_data, size=point_size, color=two_colors[2]) +
  geom_errorbar(mapping = aes(auroc_eval, ymin=metric_lower, ymax=metric_upper), data = plot_data, width=bar_size, color=two_colors[2]) +
  geom_segment(aes(x = x1, y = lower_y, xend = lower_x2, yend = lower_y), data = segment_data, color=two_colors[2], linetype="dashed") +
  geom_segment(aes(x = lower_x2, y = lower_y, xend = lower_x2, yend = -Inf), data = segment_data, color=two_colors[2], linetype="dashed") +
  geom_segment(aes(x = x1, y = upper_y, xend = upper_x2, yend = upper_y), data = segment_data, color=two_colors[2], linetype="dashed") +
  geom_segment(aes(x = upper_x2, y = upper_y, xend = upper_x2, yend = -Inf), data = segment_data, color=two_colors[2], linetype="dashed") +
  theme_bw() +
  theme(
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size)
  ) + 
  NULL


```

```{r toy_example, fig.width=5, fig.height=2.2, fig.cap="\\label{fig:toy_example}Signal curve example. The two panels are united by the same sequence of models used to construct the quality grid. The AUROC serves as the common reference scale on the x-axis."}

ci_plot

```

## 5. Binormal model

```{r gaussian PRC curve}
#-----
# binormal_ppv
#
# Gives PPV at a given TPR assuming positive scores are drawn from a N(mu1, 1) distribution and negative scores drawn from a N(0, 1) distribution.
#
## Arguments
#
# tpr       true positive rate. Ranges from 0 to 1
# alpha     prevalence
# mu1       mean of the positive class
#
# Notes: This function assume standard normal distributions with equal variances and the negative class centered at zero. See Brodersen et al. for a more general formulation.

binormal_ppv <- function(tpr, alpha, mu1) {
  ppv <- (alpha*tpr)/(alpha*tpr + (1-alpha)*( 1- pnorm(qnorm(1-tpr, mean = mu1))))
  ppv
}

#-----
# binormal_auprc
#
# Use numerical integration to calculate AUPRC for positive scores drawn from a N(mu1, 1) distribution and negative scores drawn from a N(0, 1) distribution.
#
## Arguments
#
# auroc     auroc to determine distance between the two distributions
# alpha     prevalence
# nsim      number of grid points for numerical integration
#
binormal_auprc <- function(auroc, alpha, nsim=1000) {
  
  # find shift parameter
  mu1 <- sqrt(2)*qnorm(auroc)
  
  # tpr sequence
  tpr <- seq(1/nsim, 1, length.out=nsim)
  
  # find average precision
  auprc <- mean(binormal_ppv(tpr = tpr, alpha = alpha, mu1 = mu1))
  auprc
    
}

```


```{r approximate AUPRC across grid using binormal model, eval=FALSE}
# form tuning grid
prevalence <- c(.01, .05, .10, .20, .30, .40, .50)
auroc_values <- seq(0.5, .99995, length.out = 10000)

tune_grid <- expand.grid(auroc=auroc_values, prevalence=prevalence)

gauss_results <- data.frame(
  prevalence = tune_grid$prevalence,
  pop_auroc = tune_grid$auroc,
  auprc = NA
)

# array for apply function
auroc_values <- matrix(auroc_values, ncol = 1)

for(i in 1:length(prevalence) ) {
  
  # set parameters
  v <- prevalence[i]
  
  # estimate auprs
  new_auprc <- apply(auroc_values, 1, binormal_auprc, alpha=v)
  
  # store results
  gauss_results[gauss_results$prevalence==v, 3] <- new_auprc
  
}

# save results
write_csv(gauss_results, file = "gauss_auprc_grid.csv")

```


```{r functions for binormal simulation}
# overlap functions
source("dist_overlap.R")

# binormal signal curve
auprc_grid <- read_csv("gauss_auprc_grid.csv")

# functions to recover estimates
est_auroc <- function(scores, outcomes) {
  estimate <- PRROC::roc.curve(scores.class0 = scores, weights.class0 = outcomes)$auc
  estimate
}

est_auprc <- function(scores, outcomes) {
  estimate <- PRROC::pr.curve(scores.class0 = scores, weights.class0 = outcomes)$auc.davis.goadrich
  estimate
}

```


```{r binormal simulation study, eval=FALSE}
# number of repeats
nrepeats <- 3

# number of simulations: limited by vector allocation size 
nsim <- 10000

# batches: number of packets to break up nsim
nbatches <- 1

set.seed(1)
# fix total number of cases or number of negative cases
N <- 1000

# check max vector allocation
if (nsim*N/nbatches > 1e+08) {
  stop("Cannot allocate vector of that size. Try increasing the number of batches/reducing the batch size.")
}

# set uncertainty interval range
CIrange <- 0.95

# simulation parameters
prevalence <- c(.01, .05, .10, .20, .30, .40, .50)
pop_auroc <- c(0.65, 0.75, 0.85, 0.95)

sim_grid <- expand.grid(pop_auroc, prevalence)
names(sim_grid) <- c("pop_auroc", "prevalence")

# bootstrap data store
gauss_results <- data.frame(
  nrepeat = NA,
  prevalence = sim_grid$prevalence,
  pop_auroc = sim_grid$pop_auroc,
  roc_est = NA,
  roc_est_lower = NA,
  roc_est_upper = NA,
  pr_est = NA,
  pr_est_lower = NA,
  pr_est_upper = NA
)

# placeholder to store each repeat
gauss_store <- data.frame(
  nrepeat = NA,
  prevalence = NA,
  pop_auroc = NA,
  roc_est = NA,
  roc_est_lower = NA,
  roc_est_upper = NA,
  pr_est = NA,
  pr_est_lower = NA,
  pr_est_upper = NA
)


start <- Sys.time()

for (nr in 1:nrepeats) {
  
  counter <- 1
  
  for (v in prevalence) {
    
    for (pa in pop_auroc) {
      
      nsize <- nsim/nbatches
      aurocs <- NULL; auprcs <- NULL
      
      for(nb in 1:nbatches) {
        
        # apply prevalence
        #n_ones <- round(n_zeros*v/(1-v))
        n_ones <- round(v*N)
        n_zeros <- N - n_ones
        
        # simulation outcome vector
        new_outcome = c(rep(1, n_ones), rep(0, n_zeros))
        
        # simulate values
        model_vecs <- gauss_overlap(auroc = pa, 
                                    n_ones = nsize*n_ones, 
                                    n_zeros = nsize*n_zeros)
        
        one_scores <- model_vecs[model_vecs$outcome==1,2] %>% 
          matrix(ncol = nsize) %>% 
          as.data.frame()
        
        zero_scores <- model_vecs[model_vecs$outcome==0,2] %>% 
          matrix(ncol = nsize) %>% 
          as.data.frame()
        
        new_scores <- bind_rows(one_scores, zero_scores)
        
        # find estimates
        new_aurocs <- apply(new_scores, 2, est_auroc, outcomes = new_outcome)
        new_auprcs <- apply(new_scores, 2, est_auprc, outcomes = new_outcome)
        
        # store estimates
        aurocs <- c(aurocs, new_aurocs)
        auprcs <- c(auprcs, new_auprcs)
        
      }
      
      # expectations
      E_roc <- mean(aurocs)
      E_pr <- mean(auprcs)
      
      # uncertainty intervals
      alpha_one_side <- (1 - CIrange)/2
      lower_quantile = alpha_one_side; upper_quantile = 1 - alpha_one_side
      ci_roc <- quantile(aurocs, probs = c(lower_quantile, upper_quantile))
      ci_pr <- quantile(auprcs, probs = c(lower_quantile, upper_quantile))
      
      # store results
      gauss_results[counter, -c(1,2,3)] <- c(E_roc, c(ci_roc), E_pr, c(ci_pr))
      
      cat(paste(counter, "\n"))
      counter <- counter+1
    }
    
  }
  
  gauss_results$nrepeat <- nr
  gauss_store <- bind_rows(gauss_store, gauss_results)
  
}

end <- Sys.time()
end - start

gauss_store <- gauss_store[-1,]

write_csv(gauss_store, file = paste0("gauss_", N, "N_", nsim, "nsim_", nrepeats, "nrepeats.csv"))


```


```{r map auprc to auroc}
#-----
# auprc_to_auroc
#
# For a given AUPRC score, find the corresponding auroc score from the function approximation.
auprc_to_auroc <- function(auprc, grid_fun) {
  
  out <- NULL
  
  for (pr in auprc) {
    closest_ind <- which.min(abs(100*pr - 100*grid_fun$auprc))
    pr_auroc <- grid_fun$pop_auroc[closest_ind]
    out <- c(out, pr_auroc)
  }
  
  out
}

```


```{r N10k simulation data}
# read-in data
gauss_store <- read_csv("gauss_10000N_10000nsim_3nrepeats.csv")

# find average estimates across runs
gauss_results <- gauss_store %>%
  group_by(prevalence, pop_auroc) %>% 
  summarise(across(everything(), ~mean(.x)) ) %>% 
  select(-nrepeat)

# add population auprc to results
auprc_key <- auprc_grid %>% 
  unite(col = "prev_auroc", prevalence:pop_auroc, sep="_") 

gauss_results <- gauss_results %>% 
  unite(col = "prev_auroc", prevalence:pop_auroc, sep="_", remove=FALSE)

gauss_results <- left_join(gauss_results, auprc_key, "prev_auroc") %>% 
  select(-prev_auroc) %>% 
  rename(pop_auprc = auprc) %>% 
  relocate(pop_auprc, .before = pr_est)

# map auprc to auroc
prevalence <- unique(gauss_results$prevalence)
pr_mean <- NULL
pr_lower <- NULL
pr_upper <- NULL

for(v in prevalence) {
  
  grid_fun <- auprc_grid %>% 
    filter(prevalence==v)
  
  # auprc to auroc: note that it is only to 4-decimal places of precision since grid contains 1000 points
  new_mean <- auprc_to_auroc(gauss_results$pr_est[gauss_results$prevalence==v], grid_fun = grid_fun)
  new_lower <- auprc_to_auroc(gauss_results$pr_est_lower[gauss_results$prevalence==v], grid_fun = grid_fun)
  new_upper <- auprc_to_auroc(gauss_results$pr_est_upper[gauss_results$prevalence==v], grid_fun = grid_fun)
  
  pr_mean <- c(pr_mean, new_mean)
  pr_lower <- c(pr_lower, new_lower)
  pr_upper <- c(pr_upper, new_upper)
  
}

# append to main grid
gauss_results$pr_est_roc <- pr_mean
gauss_results$pr_lower_roc <- pr_lower
gauss_results$pr_upper_roc <- pr_upper

# create data for plotting
plot_data <- gauss_results %>% 
  mutate(
    auroc_bias = roc_est - pop_auroc,
    auprc_bias = pr_est - pop_auprc,
    auprc_roc_bias = pr_est_roc - pop_auroc,
    auroc_CI_width = roc_est_upper - roc_est_lower, 
    auprc_CI_width = pr_upper_roc - pr_lower_roc,
    width_delta = auprc_CI_width - auroc_CI_width
  ) %>% 
  select(
    prevalence,
    pop_auroc,
    #auroc_bias,
    #auprc_bias,
    #auprc_roc_bias,
    roc_est_lower,
    roc_est_upper,
    pr_lower_roc,
    pr_upper_roc,
    auroc_CI_width,
    auprc_CI_width,
    width_delta
  ) %>% 
  mutate(
    # Use the Hosmer & Lemeshow rule of thumb: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/roc
    model_quality = case_when(
      pop_auroc==.65 ~ "Poor",
      pop_auroc==.75 ~ "Fair",
      pop_auroc==.85 ~ "Good",
      pop_auroc==.95 ~ "Excellent"
    ),
    model_quality = factor(model_quality, levels = c("Poor", "Fair", "Good", "Excellent")),
    prevalence = factor(prevalence),
    ci_ratio = auprc_CI_width/auroc_CI_width - 1
  )

  

```


```{r N10k relative comparison plot}
pop_auroc <- unique(plot_data$pop_auroc)

plot_relative <- ggplot(plot_data, aes(prevalence, ci_ratio, fill=prevalence)) +
  facet_wrap( ~ model_quality, nrow = 1) +
  geom_col() +
  scale_x_discrete(labels = c(".01", ".05", ".10", ".20", ".30", ".40", ".50")) +
  ylab(~ paste(kappa["PRC"], "/", kappa["ROC"], " - 1")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = rr_axis_title_size), 
    axis.text = element_text(size = rr_axis_text_size)
    ) +
  guides(fill="none") +
  ylim(c(-.15, .35)) +
  NULL


```


```{r auc_ci function, include=FALSE}
#----
# se_auc
#
# Standard error for the area under the empirical ROC curve.
#
## Arguments
#
# theta           the population AUROC value
# n1              number of positive cases
# n0              number of negative cases
#
# References: See Hanley and McNeil 1982

se_auc <- function(theta, n1, n0) {
  
  # assume negative exponential model for these quantities:
  q1 <- theta/(2 - theta)
  q2 <- 2*theta^2/(1 + theta)
  
  se_w <- sqrt(
    (theta*(1 - theta) + (n1 - 1)*(q1 - theta^2) + (n0 - 1)*(q2 - theta^2))/(n1*n0)
  )
  
  se_w
  
}

```


```{r data for AUROC SE plot}
# add large sample, normal approximation CIs to grid
N <- 10000
n1 <- N*auprc_grid$prevalence
n0 <- N - n1

SE_data <- se_auc(theta = auprc_grid$pop_auroc, n1 = n1, n0 = n0)
auprc_grid$SE_AUROC <- SE_data


```


```{r AUPRC signal curve plot}
# define prevalence as a factor
auprc_grid <- auprc_grid %>% 
  mutate(
    prevalence = factor(prevalence)
  )

# find slope for prevalence 0.5 curve
fit50 <- lm(auprc ~ pop_auroc, data = auprc_grid, subset = auprc_grid$prevalence=="0.5")

# construct data used for simulation
eval_data <- auprc_grid[round(auprc_grid$pop_auroc,5) %in% pop_auroc, ]

eval_data <- eval_data %>% 
  mutate(
    pop_auroc = round(pop_auroc,5),
    model_quality = case_when(
      pop_auroc==.65 ~ "Poor",
      pop_auroc==.75 ~ "Fair",
      pop_auroc==.85 ~ "Good",
      pop_auroc==.95 ~ "Excellent"
    ),
    model_quality = factor(model_quality, levels = c("Poor", "Fair", "Good", "Excellent"))
    )

gauss_SC <- ggplot(auprc_grid, aes(pop_auroc, auprc, color=prevalence)) + 
  geom_path(size = 0.3) +
  geom_point(data = eval_data, 
             aes(pop_auroc, 
                 auprc, 
                 shape=model_quality
                 ), 
             size=1.5
             ) +
  xlab("AUROC") +
  ylab("AUPRC") +
  labs(shape = "model quality") +
  theme_bw() +
  theme(
    legend.box = "horizontal",
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size),
    legend.title = element_text(size=legend_title_size),
    legend.text = element_text(size=legend_text_size)
  ) +
  NULL


```


```{r  AUROC resolving power plot}
# power plot data: limit AUROC range
#power_data <- auprc_grid %>%  
#  filter(pop_auroc <= .95) %>%
#  mutate(
#    prevalence = factor(prevalence)
#  )

SE_plot <- ggplot(auprc_grid, aes(pop_auroc, SE_AUROC, color=prevalence)) + 
  geom_path(size=.3) +
  xlab("AUROC") +
  ylab("Standard Error") +
  theme_bw() +
  theme(
    legend.box = "horizontal",
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size),
    legend.title = element_text(size=legend_title_size),
    legend.text = element_text(size=legend_text_size)
    ) +
  NULL

####
# normal approximate CIs

zstat <- qnorm(1-.05/2)
N <- 10000

# AUROC = .65 and prevalence = .01
theta <- 0.65
prev <- .01
n1 <- prev*N
n0 <- N - n1

# normal approx CI
condition_SE <- se_auc(theta, n1, n0)
normal_auroc65CI <- c(theta - zstat*condition_SE, theta + zstat*condition_SE)

# simulation CI
sim_auroc65CI <- plot_data %>% 
  filter(prevalence == as.character(prev), pop_auroc==theta)

sim_auroc65CI <- c(sim_auroc65CI$roc_est_lower, sim_auroc65CI$roc_est_upper)


# AUROC = .95 and prevalence = .01
theta <- 0.95
prev <- .01
n1 <- prev*N
n0 <- N - n1

# normal approx CI
condition_SE <- se_auc(theta, n1, n0)
normal_auroc95CI <- c(theta - zstat*condition_SE, theta + zstat*condition_SE)

# simulation CI
sim_auroc95CI <- plot_data %>% 
  filter(prevalence == as.character(prev), pop_auroc==theta)

sim_auroc95CI <- c(sim_auroc95CI$roc_est_lower, sim_auroc95CI$roc_est_upper)


```

```{r gauss_SC, fig.width=4.5, fig.height=2.2, fig.cap="\\label{fig:gauss_SC}Mapping between AUROC and AUPRC for the binormal model."}
gauss_SC

```

```{r N10k_compare, fig.width=5, fig.height=2, fig.cap="\\label{fig:N10k_compare}Relative metric resolution by outcome prevalence and model quality for a binormal model with a sample size of N = 10,000. At each level of model quality 10,000 simulations are taken from the sampling model. Confidence limits are the .025 and .975 quantile values of the simulation samples."}
plot_relative

```

```{r SE_plot, fig.width=4, fig.height=2.2, fig.cap="\\label{fig:SE_plot}The relationship between AUROC and its standard error for different levels of outcome prevalence. The standard error is found using Hanley and McNeil (1982)'s approximation formula."}
SE_plot

```

## 6. Empirical sampling models

```{r import data}
# see source folder for scripts that built this data set
load("diabetes_post_raw.RData")

```

```{r prep data for XGBoost}
# separate ID
patient_nbr <- dataset %>% 
  select(patient_nbr)

# select outcome
dataset <- dataset %>% 
  rename(
    DV = outcome
  ) %>%
  mutate(
    DV = ifelse(DV=="readmitted", 1, 0)
  ) %>% 
  select(
    -patient_nbr,
  ) %>% 
  relocate(DV)

# remove DV for R code
DV <- dataset$DV
dataset <- select(dataset, -DV)

```

```{r create model matrix}
# use caret functions
dmy <- dummyVars( ~ ., data = dataset, fullRank = TRUE)
mod_mat <- predict(dmy, newdata = dataset)

```

```{r 5-fold data splitting, include=FALSE}
set.seed(1)
kfolds <- 5
fold_list <- createFolds(as.factor(DV), k = kfolds)

# check the number of events per fold: want about 100.
for(j in 1:length(fold_list)) {
  print(sum(DV[fold_list[[j]]]))
}

```

```{r, include=FALSE}
###
# cross-validation

# store test results
test_results <- data.frame(DV=NA, ylogit=NA)

for(kf in 1:kfolds) {
  
  # cross-validation
  next_test <- kf
  next_train <- setdiff(1:kfolds, kf)
  
  train_index <- unlist(fold_list[next_train])
  test_index <- unlist(fold_list[next_test])
  
  ###
  # train model
  
  # training data
  train_mat <- mod_mat[train_index,]
  train_DV <- DV[train_index]
  train_data <- cbind(train_DV, train_mat) %>% 
    as.data.frame()
  
  glm_mod <- glm(train_DV ~ ., data = train_data, family = binomial())
  
  ###
  # test results
  
  # test data
  test_mat <- mod_mat[test_index,]
  test_DV <- DV[test_index]
  test_data <- as.data.frame(test_mat)
  
  ylogit <- predict(glm_mod, newdata = test_data)
  
  # store results 
  new_results <- data.frame(DV = test_DV, ylogit = ylogit)
  test_results <- bind_rows(test_results, new_results)
  cat("*")
  
}

test_results <- test_results[-1,]

test_results <- test_results %>% 
  mutate(
    class = ifelse(DV==1, "positive", "negative"),
    class = factor(class, levels = c("negative", "positive"))
  )

```


```{r descriptive statistics}
# prevalence
readmit_prevalence <- mean(test_results$DV)

# average logit in each group
test_summary <- test_results %>% 
  mutate(
    est_prob = boot::inv.logit(ylogit)
  ) %>% 
  group_by(class) %>% 
  summarise(
    Npats = n(),
    mean_logit = mean(ylogit),
    sd_logit = sd(ylogit),
    mean_prob = mean(est_prob)
  )

# Sample AUROC
test_auroc <- PRROC::roc.curve(scores.class0 = test_results$ylogit, weights.class0 = test_results$DV)

# Sample AUPRC
test_auprc <- PRROC::pr.curve(scores.class0 = test_results$ylogit, weights.class0 = test_results$DV)


```


```{r plot logit distributions}
#-----
# risk distribution plot

logit_distribution <- ggplot(test_results, aes(ylogit, color=class)) +
  geom_density() + 
  geom_rug(data = subset(test_results, DV==1), sides = "t", alpha=2/5) +
  geom_rug(data = subset(test_results, DV==0), sides = "b", alpha=2/5) +
  scale_color_manual(values = two_colors) +
  labs(
    y = "density",
    x = "estimated logit",
    color = "class"
  ) +
  theme_bw() +
  theme(
    legend.box = "horizontal",
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size),
    legend.title = element_text(size=legend_title_size),
    legend.text = element_text(size=legend_text_size)
  ) +
  NULL
  
```


```{r qqnorm plots}
qq_distribution <- ggplot(test_results, aes(sample=ylogit, color=class)) +
  facet_grid(~class) +
  stat_qq(size=.8) +
  stat_qq_line() +
  scale_color_manual(values = two_colors) +
  labs(x = "theoretical", y="sample") +
  theme_bw() +
  theme(
    legend.box = "horizontal",
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size),
    legend.title = element_text(size=legend_title_size),
    legend.text = element_text(size=legend_text_size)
  ) +
  NULL
  
```


```{r step 1 create baseline model}
# set-up baseline model
baseline_model <- test_results %>% 
  select(
    outcome = DV,
    risk_score = ylogit,
  ) %>% 
  arrange(
    desc(risk_score)
  )

```


```{r step 2 grid formation}
#----
# build grid function
#
# Function to build signal curve grid from a baseline model
#
## Arguments
# 
# baseline_model    empirical distribution of effects from a baseline model
# auroc_pos         auroc grid distance above baseline
# auroc_neg         auroc grid distance below baseline
# one_auroc_step    grid step size in auroc units
# exact_shift       find exact distribution shift at each step? currently only finds exact step for first step.

build_grid <- function(baseline_model, auroc_pos=.2, auroc_neg=.1, one_auroc_step=.001, exact_shift=FALSE) {
  
  # positive and negative effects from the baseline model
  pos_effects <- baseline_model$risk_score[baseline_model$outcome==1]
  neg_effects <- baseline_model$risk_score[baseline_model$outcome==0]
  
  # find the total number of adjacent switches
  n_pos <- sum(baseline_model$outcome) 
  n_neg <- nrow(baseline_model) - n_pos
  total_switches <- n_pos*n_neg
  
  # number of switches for one increment of auroc
  one_step_switches <- total_switches*one_auroc_step
  
  # grid steps in positive and negative direction
  pos_steps <- auroc_pos/one_auroc_step + 1
  neg_steps <- auroc_neg/one_auroc_step + 1
  
  # outer-product distance matrix
  # negative difference means "out-of-order" 
  # positive difference means "in-order"
  dist_mat <- outer(pos_effects, neg_effects, FUN = "-")
  
  ####
  # positive direction grid
  
  # shift to achieve number_switches in the positive direction
  out_of_order <- sort(-1*dist_mat[dist_mat<0], decreasing=FALSE)
  logit_shift <- out_of_order[one_step_switches]
  
  # start grid at baseline
  pos_grid <- data.frame(steps=1:pos_steps, auroc=NA, auprc=NA)
  
  shift_model <- baseline_model
  
  for(i in 1:pos_steps) {
    
    # calculate  and store auroc and auprc
    auroc <- PRROC::roc.curve(scores.class0 = shift_model$risk_score, weights.class0 = shift_model$outcome)$auc
    
    auprc <- PRROC::pr.curve(scores.class0 = shift_model$risk_score, weights.class0 = shift_model$outcome)$auc.davis.goadrich
    
    pos_grid[i,-1] <- c(auroc, auprc)  
    
    # shift grid by adding to positive class
    shift_model$risk_score[shift_model$outcome==1] <- shift_model$risk_score[shift_model$outcome==1] + logit_shift
    
    # if exact, find the updated distance
    # will be computationally expensive so probably not worth it
    if(exact_shift) {stop("feature not implemented")}
    
  }
  
  ####
  # negative direction grid
  
  # shift to achieve number_switches in the negative direction
  out_of_order <- sort(dist_mat[dist_mat>0], decreasing=FALSE)
  logit_shift <- out_of_order[one_step_switches]
  
  # start grid at baseline
  neg_grid <- data.frame(steps=1:neg_steps, auroc=NA, auprc=NA)
  
  shift_model <- baseline_model
  
  for(i in 1:neg_steps) {
    
    # calculate  and store auroc and auprc
    auroc <- PRROC::roc.curve(scores.class0 = shift_model$risk_score, weights.class0 = shift_model$outcome)$auc
    
    auprc <- PRROC::pr.curve(scores.class0 = shift_model$risk_score, weights.class0 = shift_model$outcome)$auc.davis.goadrich
    
    neg_grid[i,-1] <- c(auroc, auprc)  
    
    # shift grid by adding to positive class
    shift_model$risk_score[shift_model$outcome==1] <- shift_model$risk_score[shift_model$outcome==1] - logit_shift
    
    # if exact, find the updated distance
    # will be computationally expensive so probably not worth it
    if(exact_shift) {stop("feature not implemented")}
    
  }
  
  # create final grid
  neg_grid <- neg_grid[-1,]
  pos_grid <- arrange(pos_grid, desc(steps))
  
  model_grid <- bind_rows(pos_grid, neg_grid) %>% 
    select(-steps)
  
  model_grid
  
}


```


```{r form empirical grid, eval=FALSE}
empirical_grid <- build_grid(baseline_model = baseline_model, one_auroc_step = .0005, auroc_pos = 0.4, auroc_neg = 0.1)
write_csv(empirical_grid, "empirical_grid.csv")

```


```{r step 3 noise estimation, eval=FALSE}
# indices for stratified sampling
row_index <- 1:nrow(baseline_model)
pos_index <- row_index[baseline_model$outcome==1]
neg_index <- setdiff(row_index, pos_index)

# apply stratified sampling to create bootstrap samples
nsim <- 10000
store_boot <- data.frame(sim = 1:nsim, auroc=NA, auprc=NA)

set.seed(1)
for(i in 1:nsim) {
  
  # stratified sampling
  pos_sample <- sample(pos_index, size = length(pos_index), replace = TRUE)
  neg_sample <- sample(neg_index, size = length(neg_index), replace = TRUE)
  
  boot_index <- c(pos_sample, neg_sample)
  boot_sample <- baseline_model[boot_index, ]
  
  # find metric values
  new_auroc <- PRROC::roc.curve(scores.class0 = boot_sample$risk_score, weights.class0 = boot_sample$outcome)$auc
  
  new_auprc <- PRROC::pr.curve(scores.class0 = boot_sample$risk_score, weights.class0 = boot_sample$outcome)$auc.davis.goadrich
  
  store_boot[i,-1] <- c(new_auroc, new_auprc)
  
}

write_csv(store_boot, file = "store_boot.csv")

```


```{r step 4 compare}
# import model grid
empirical_grid <- read_csv("empirical_grid.csv")

# import bootstrap data
store_boot <- read_csv("store_boot.csv")

# bootstrap bias
auroc_bias <- mean(store_boot$auroc) - test_auroc$auc
point_auprc <- mean(store_boot$auprc)
auprc_bias <- point_auprc - test_auprc$auc.davis.goadrich

# 95 percentile bootstrap interval
auroc_CI <-  quantile(store_boot$auroc, probs = c(.025,.975))
auprc_CI <- quantile(store_boot$auprc, probs = c(.025,.975))

# map auprc to auroc units
names(empirical_grid)[1] <- "pop_auroc"
auprc_CI_convert <- data.frame(
  point_estimate = auprc_to_auroc(point_auprc, empirical_grid),
  lower_ci = auprc_to_auroc(auprc_CI[1], empirical_grid),
  upper_ci = auprc_to_auroc(auprc_CI[2], empirical_grid)
)

auprc_to_auroc_bias <- auprc_CI_convert$point_estimate - test_auroc$auc

####
# relative difference

# AUPRC is about 75% wider
rel_diff <- (auprc_CI_convert[3] - auprc_CI_convert[2])/(auroc_CI[2] - auroc_CI[1]) - 1

# relative resolving power
relative_width = round(rel_diff,4)
names(relative_width) <- NULL

####
# absolute difference
abs_diff <- (auprc_CI_convert[3] - auprc_CI_convert[2]) - (auroc_CI[2] - auroc_CI[1])



```


```{r empirical simulation results}
bias = round(c(auroc_bias, auprc_bias, auprc_to_auroc_bias),6)

lower_ci = round(c(auroc_CI[1], auprc_CI[1], auprc_CI_convert$lower_ci), 4)
  
upper_ci = round(c(auroc_CI[2], auprc_CI[2], auprc_CI_convert$upper_ci), 4)


sim_results_summary <- data.frame(
  metric = c("AUROC", "AUPRC", "AUPRC to AUROC"),
  #Bias = bias,
  'Lower CI' = lower_ci,
  'Upper CI' = upper_ci,
  kappa = upper_ci - lower_ci
)

sim_results_summary$kappa[2] <- NA

sim_results_summary <- sim_results_summary %>% 
  mutate(
    resolving_power = 1/kappa
  )

```


```{r empirical signal curve plot}
# point of interest
auroc <- PRROC::roc.curve(scores.class0 = baseline_model$risk_score, weights.class0 = baseline_model$outcome)$auc

auprc <- PRROC::pr.curve(scores.class0 = baseline_model$risk_score, weights.class0 = baseline_model$outcome)$auc.davis.goadrich

baseline_values <- data.frame(auroc=auroc, auprc=auprc)

# create plot
empirical_plot  <- ggplot(empirical_grid, aes(pop_auroc, auprc)) + 
  geom_path() +
  geom_point(data = baseline_values, aes(auroc, auprc), size=1.5) +
  xlab("AUROC") +
  ylab("AUPRC") +
  theme_bw() +
  theme(
    legend.box = "horizontal",
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size),
  ) +
  NULL


```

```{r risk_distrib, fig.width=5, fig.height=3, fig.cap="\\label{fig:risk_distrib}Top panel: Estimated logit effects with kernel density estimates for the positive and negative class. Bottom panel: Q-Q norm plots of sample versus theoretical quantiles. Straight lines pass through the 1st and 3rd quartiles."}

ggarrange(plots = list(logit_distribution, qq_distribution), nrow=2)

```

```{r empirical_SC, fig.width=2.5, fig.height=1.5, fig.cap="\\label{fig:empirical_RC}An empirical signal curve from the readmissions data logistic regression model. The black point shows baseline performance. The curve is constructed by incrementing positive scores above and below the baseline distribution."}

empirical_plot

```

```{r empirical_summary}
sim_results_summary$resolving_power <- round(sim_results_summary$resolving_power,1)

# format names to be Latex compatible
names(sim_results_summary) <- c("metric", "Lower CI", "Upper CI", "$\\kappa$", "resolving power")

table_title <- paste0()

kableExtra::kbl(sim_results_summary, caption = "Simulation results summary. Lower and upper CI bounds are for 95 percent confidence intervals.", escape = FALSE, align = "c") %>% 
  row_spec(0,bold=TRUE) %>% 
  column_spec(1, bold = TRUE)
  
```


## 7. Empirical validation

```{r create diabetes model matrix}
# reload diabetes data
load("diabetes_post_raw.RData")

# select outcome
dataset <- dataset %>% 
  rename(
    DV = outcome
  ) %>%
  mutate(
    DV = ifelse(DV=="readmitted", 1, 0)
  ) %>% 
  select(
    -patient_nbr,
  ) %>% 
  relocate(DV)

# remove DV for R code
DV <- dataset$DV
dataset <- select(dataset, -DV)

# use caret functions
dmy <- dummyVars( ~ ., data = dataset, fullRank = TRUE)
mod_mat <- predict(dmy, newdata = dataset)

```


```{r metric_calc}
#----
# metric_calc
#
# helper to find the comparison metric value when applied to effect distribution.
#
## arguments
#
# risk_score          risk scores assigned to each case
# outcome             each cases outcome
# metric_name         name of the comparison metric
# severity.ratio      relative cost of positive versus negative class for the H-measure. Leaving to default uses the relative class proportions.

metric_calc <- function(risk_score, outcome, metric_name, severity.ratio=NA) {
  
  require(PRROC)
  
  allowed_metrics = c("auroc", "auprc", "hmeasure")
  
  if (! (metric_name %in% allowed_metrics) ) {
    stop(paste(metric_name, "currently not supported"))
  }
  
  if ( metric_name == "auroc" ) {
    value <- PRROC::roc.curve(scores.class0 = risk_score, weights.class0 = outcome)$auc
  } else if ( metric_name == "auprc" ) {
    value <- PRROC::pr.curve(scores.class0 = risk_score, weights.class0 = outcome)$auc.davis.goadrich
  } else if ( metric_name == "hmeasure" ) {
    value <- hmeasure::HMeasure(outcome, risk_score, severity.ratio = severity.ratio)$metrics$H
  }
  
  value

  }

```

```{r resolving power linear approximation function}
#----
# rpower
#
# Estimate resolving power from an empirical distribution of risk scores
#
## Arguments
#
# compare_metric      metric to compare to AUROC
# baseline_effects    empirical distribution of effects from a baseline model
# auroc_step          grid step size in auroc units
# auroc_range         2 element vector giving the upper and lower bounds of the signal curve segment.
# number_switches     grid step size in number of resolved pairs
# noise_sim           number of samples to estimate noise distribution.
#
# Must specify auroc_step or number_switches. Need an adequate number of samples in the step size to smooth out sampling variance in how the metric changes. For simulation quantile CIs probably want to use noise_sim = 10,000. For approximate CIs noise_sim = 500 should be sufficient.

rpower <- function(compare_metric, baseline_effects, severity.ratio=NA, linear_approx=TRUE, auroc_step=NULL, auroc_range=NULL, number_resolved=NULL, noise_sim = 500) {
  
  auroc_step_test <- is.null(auroc_step)
  number_resolved_test <- is.null(number_resolved)
  
  if (!auroc_step_test & !number_resolved_test) {
    stop("must specify either auroc_step or number_resolved argument")
  }
  
  if (linear_approx & !is.null(auroc_range)) {
    warning("since linear approximation selected, auroc_range argument ignored.")
  } else{
    auroc_neg = auroc_range[1]
    auroc_pos = auroc_range[2]
  }
  
  # positive and negative effects from the baseline model
  pos_effects <- baseline_effects$risk_score[baseline_effects$outcome==1]
  neg_effects <- baseline_effects$risk_score[baseline_effects$outcome==0]
  
  # find the total number of adjacent switches
  n_pos <- sum(baseline_effects$outcome) 
  n_neg <- nrow(baseline_effects) - n_pos
  total_switches <- n_pos*n_neg
  
  if(number_resolved_test) {
    number_resolved <- round(total_switches*auroc_step)
  }
  
  ####
  # outer product distance matrix
  
  # NOTE: This approach will not scale. Will need to find an approximate solution.
  
  # negative difference means "out-of-order" 
  # positive difference means "in-order"
  dist_mat <- outer(pos_effects, neg_effects, FUN = "-")
  
  # shift positive direction
  out_of_order <- sort(-1*dist_mat[dist_mat<0], decreasing=FALSE)
  pos_shift <- out_of_order[number_resolved]
  rm(out_of_order)
  
  # shift negative direction: undo "in-order" score
  in_order <- sort(dist_mat[dist_mat>0], decreasing=FALSE)
  neg_shift <- in_order[number_resolved]
  rm(in_order)
  
  #----
  # linear approximation method
  
  # linear signal approximation
  if (linear_approx) {
    
    baseline_auroc <- metric_calc(baseline_effects$risk_score, baseline_effects$outcome, "auroc")
    baseline_compare <- metric_calc(baseline_effects$risk_score, baseline_effects$outcome, compare_metric, severity.ratio = severity.ratio)
    
    linear_grid <- data.frame(grid_point = 1:3, auroc=NA, compare_metric=NA)
    names(linear_grid)[3] <- compare_metric
    rownames(linear_grid) <- c("step_down", "baseline", "step_up")
    
    linear_grid["baseline", "auroc"] <- baseline_auroc
    linear_grid["baseline", compare_metric] <- baseline_compare
    
    ####
    # shift grid by adding to positive class
    
    # positive shift
    shift_effects <- baseline_effects
    
    shift_effects$risk_score[shift_effects$outcome==1] <-  shift_effects$risk_score[shift_effects$outcome==1] + pos_shift
    
    shift_auroc <- metric_calc(shift_effects$risk_score, shift_effects$outcome, "auroc")
    shift_compare <- metric_calc(shift_effects$risk_score, shift_effects$outcome, compare_metric, severity.ratio = severity.ratio)
    linear_grid["step_up", -1] <- c(shift_auroc, shift_compare)
    
    # negative shift
    shift_effects <- baseline_effects
    
    shift_effects$risk_score[shift_effects$outcome==1] <-  shift_effects$risk_score[shift_effects$outcome==1] - neg_shift
    
    shift_auroc <- metric_calc(shift_effects$risk_score, shift_effects$outcome, "auroc")
    shift_compare <- metric_calc(shift_effects$risk_score, shift_effects$outcome, compare_metric, severity.ratio = severity.ratio)
    linear_grid["step_down", -1] <- c(shift_auroc, shift_compare)
    
    # linear approximation of signal curve
    signal_linear <- lm(linear_grid[,"auroc"] ~ linear_grid[, compare_metric])
    
  }
  
  ####
  # noise distribution: just find the SD of the sample and use + or - 2
  
  # indices for stratified sampling
  row_index <- 1:nrow(baseline_effects)
  pos_index <- row_index[baseline_effects$outcome==1]
  neg_index <- setdiff(row_index, pos_index)
  
  # apply stratified sampling to create bootstrap samples
  store_boot <- data.frame(sim = 1:noise_sim, auroc=NA, compare_metric=NA)
  
  for(i in 1:noise_sim) {
    
    # stratified sampling
    pos_sample <- sample(pos_index, size = length(pos_index), replace = TRUE)
    neg_sample <- sample(neg_index, size = length(neg_index), replace = TRUE)
    
    boot_index <- c(pos_sample, neg_sample)
    boot_effects <- baseline_effects[boot_index, ]
    
    # find metric values
    new_auroc <- metric_calc(boot_effects$risk_score, boot_effects$outcome, "auroc")
    new_compare <- metric_calc(boot_effects$risk_score, boot_effects$outcome, compare_metric, severity.ratio = severity.ratio)
    
    store_boot[i,-1] <- c(new_auroc, new_compare)
    
  }
  
  ####
  # calculate resolving power
  
  # auroc
  auroc_sd <- sd(store_boot$auroc)
  auroc_ci <- c(baseline_auroc - 2*auroc_sd, baseline_auroc + 2*auroc_sd)
  kappa_auroc <- auroc_ci[2] - auroc_ci[1]

  # comparison
  compare_sd <- sd(store_boot$compare_metric)
  compare_ci <- c(baseline_compare - 2*compare_sd, baseline_compare + 2*compare_sd)
  # intercept subtracts out, so only multiply by the slope
  compare_ci_auroc <- signal_linear$coefficients[2]*compare_ci
  
  raw_compare <- compare_ci[2] - compare_ci[1]
  kappa_compare <- compare_ci_auroc[2] - compare_ci_auroc[1]

  output <- data.frame(
    prevalence = mean(baseline_effects$outcome),
    number_resolved = number_resolved,
    baseline_auroc = baseline_auroc,
    baseline_compare = baseline_compare,
    kappa_auroc = kappa_auroc,
    raw_compare = raw_compare,
    kappa_compare = kappa_compare,
    slope_approx = signal_linear$coefficients[2],
    severity.ratio = severity.ratio
  )
  
  rownames(output) <- NULL
  output
  
}


```


```{r xgboost simulation experiment, eval=FALSE}
# save results
save_results = TRUE
est_power = TRUE
fname_baseline = "xgb_baseline_rpower.csv"
fname_test = "xgb_tuning_experiment.csv"

# simulation set-up
rseed = 1772 
set.seed(rseed)
nsim <- 500

# default parameters
param_list <- list(
  eta = NA,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  max_delta_step = 0,
  subsample = 1,
  colsample_bytree = 1,
  lambda = 1,
  alpha = 0
)

# baseline parameters
rounds_baseline = 25
eta_baseline = 0.1

# store training estimates of resolving power
training_power <- data.frame(iter=1:nsim, kappa_roc=NA, kappa_prc=NA)

# first is the validation measure, second is test performance
test_results <- data.frame(iter=1:nsim,
                           top_agree=NA,
                           roc_params=NA,
                           roc_est=NA,
                           roc_roc=NA,
                           roc_prc=NA,
                           prc_params=NA,
                           prc_est=NA,
                           prc_roc=NA,
                           prc_prc=NA
                           )

for (j in 1:nsim) {
  
  # track best model
  best_roc = 0.5
  best_roc_index = 1
  
  best_prc = .05
  best_prc_index = 1
  
  # 3-way split
  kfolds <- 20
  fold_list <- createFolds(as.factor(DV), k = kfolds)
  
  train_index <- unlist(fold_list[1:3])
  valid_index <- unlist(fold_list[4])
  test_index <- unlist(fold_list[5:kfolds])
  
  ####
  # split data
  train_mat <- mod_mat[train_index, ]
  train_DV <- DV[train_index]
  
  valid_mat <- mod_mat[valid_index, ]
  valid_DV <- DV[valid_index]
  
  test_mat <- mod_mat[test_index, ]
  test_DV <- DV[test_index]
  
  ####
  # baseline resolving power
  
  if (est_power) {
    param_list$eta <- eta_baseline
    
    # baseline model
    xgb_train <- xgboost::xgboost(train_mat, train_DV, params = param_list, nrounds = rounds_baseline, booster = "gbtree", objective = "binary:logitraw", verbose = 0)
    
    # baseline effects
    xgb_valid <- predict(xgb_train, valid_mat)
    baseline_effects <- data.frame(outcome = valid_DV, risk_score = xgb_valid)
    
    prc_rpower <- rpower(compare_metric = "auprc",
                         baseline_effects = baseline_effects,
                         auroc_step = .001,
                         noise_sim = 500)
    
    training_power[j,-1] <- c(prc_rpower$kappa_auroc, prc_rpower$kappa_compare)
    
  }
  
  ####
  # new tuning grid
  tune_grid <- data.frame(
    num_rounds = sample(25:150, size = 6, replace = FALSE), 
    eta = runif(6, .01, .3) 
    )
  
  for( i in 1:nrow(tune_grid) ) {
    
    param_list$eta = tune_grid$eta[i]
    next_rounds <- tune_grid$num_rounds[i]
    
    xgb_train <- xgboost::xgboost(train_mat, train_DV, params = param_list, nrounds = next_rounds, booster = "gbtree", objective = "binary:logitraw", verbose = 0)
    
    xgb_valid <- predict(xgb_train, valid_mat)
    
    # tuning results
    roc_valid <- metric_calc(xgb_valid, valid_DV, "auroc")
    prc_valid <- metric_calc(xgb_valid, valid_DV, "auprc")

    # test for best tuning result
    if (roc_valid > best_roc) {
      best_roc_index = i
      best_roc = roc_valid
      best_roc_model = xgb_train
      best_roc_params = paste(round(tune_grid$eta[best_roc_index],2), tune_grid$num_rounds[best_roc_index], sep=", ")
    }
    
    if (prc_valid > best_prc) {
      best_prc_index = i
      best_prc = prc_valid
      best_prc_model = xgb_train
      best_prc_params = paste(round(tune_grid$eta[best_prc_index],2), tune_grid$num_rounds[best_prc_index], sep=", ")
    }
    
    cat("*")
  }
  
  ####
  # see if there is a disagreement in ranks and record
  top_agree <- best_roc_index == best_prc_index
  test_results$top_agree[j] <- top_agree
  
  ####
  # fit best
  
  # one model if agreement in tuning
  if (top_agree) {
    best_scores <- predict(best_roc_model, test_mat)
    roc_test <- metric_calc(best_scores, outcome = test_DV, metric_name = "auroc")
    prc_test <- metric_calc(best_scores, outcome = test_DV, metric_name = "auprc")
    
    test_results[j, c("roc_params", "prc_params")] <- c(best_roc_params, best_prc_params)
    
    test_results[j, c("roc_est", "roc_roc", "roc_prc", "prc_est", "prc_roc", "prc_prc")] <- c(best_roc, roc_test, prc_test, best_prc, roc_test, prc_test)

  } else {
    # roc final model
    roc_scores <- predict(best_roc_model, test_mat)
    roc_test_roc <- metric_calc(roc_scores, outcome = test_DV, metric_name = "auroc")
    roc_test_prc <- metric_calc(roc_scores, outcome = test_DV, metric_name = "auprc")
  
    test_results[j, "roc_params"] <- best_roc_params
    test_results[j, c("roc_est", "roc_roc", "roc_prc")] <- c(best_roc, roc_test_roc, roc_test_prc)
  
  # prc final model
  prc_scores <- predict(best_prc_model, test_mat)
  prc_test_roc <- metric_calc(prc_scores, outcome = test_DV, metric_name = "auroc")
  prc_test_prc <- metric_calc(prc_scores, outcome = test_DV, metric_name = "auprc")
  
  test_results[j, "prc_params"] <- best_prc_params
  test_results[j, c("prc_est", "prc_roc", "prc_prc")] <- c(best_prc, prc_test_roc, prc_test_prc)
    
  }
  
  cat(paste(" ", j, "of", nsim, "iterations complete\n"))
  
}

if(save_results) {
  write_csv(training_power, file = fname_baseline)
  write_csv(test_results, file = fname_test)
}

```


```{r import simulation results}
baseline_results <- read_csv("xgb_baseline_rpower.csv")
test_results <- read_csv("xgb_tuning_experiment.csv")

```


```{r simulation train results, include=FALSE}
# Does auroc always have better resolving power
all(baseline_results$kappa_roc < baseline_results$kappa_prc)
roc_better_indicator <- baseline_results$kappa_roc < baseline_results$kappa_prc
roc_rpower_better <- sum(roc_better_indicator)

# compare resolving power estimates from the baseline model
baseline_summary <- baseline_results %>% 
  mutate(
    rpower_roc = 1/kappa_roc,
    rpower_prc = 1/kappa_prc
  ) %>% 
  summarise(
    roc_mean = mean(rpower_roc),
    roc_median = median(rpower_roc),
    prc_mean = mean(rpower_prc),
    prc_median = median(rpower_prc)
  )

# average advantage
baseline_summary$roc_mean/baseline_summary$prc_mean

# change names for a pretty table
#names(baseline_summary) <- c("ROC mean", "ROC median", "PRC mean", "PRC median")

```


```{r simulation test results, include=FALSE}
# remove trial where prc has better resolving power
test_results <- test_results %>% 
  filter(roc_better_indicator)

# proportion agreement
num_agree <- sum(test_results$top_agree)
prop_agree <- mean(test_results$top_agree)
percent_agree <- round(100*prop_agree)

# trials where the predictions disagree. Also keep only trials with kappa_roc < kappa_prc
disagree_results <- test_results %>% 
  filter(!top_agree)

# compare performance
mean(disagree_results$roc_roc > disagree_results$prc_roc)
mean(disagree_results$roc_prc > disagree_results$prc_prc)

####
# binomial test

# number better when tuning with ROC: hypothesis tests
roc_better <- sum(disagree_results$roc_roc > disagree_results$prc_roc)
binom.test(roc_better, nrow(disagree_results), p = 0.5)
roc_better_percent <- round(100*mean(disagree_results$roc_roc > disagree_results$prc_roc))

prc_better <- sum(disagree_results$roc_prc > disagree_results$prc_prc)
binom.test(prc_better, nrow(disagree_results), p = 0.5)
prc_better_percent <- round(100*mean(disagree_results$roc_prc > disagree_results$prc_prc))

####
# t-tests:

# independent samples
t.test(disagree_results$roc_roc, disagree_results$prc_roc)
t.test(disagree_results$roc_prc, disagree_results$prc_prc)

# dependent samples
t.test(disagree_results$roc_roc - disagree_results$prc_roc)
t.test(disagree_results$roc_prc - disagree_results$prc_prc)


```


```{r test_summary}
# shows that when there is disagreement, the ROC selection does about as good a job as when agreement while the PRC takes a hit.
test_summary <- test_results %>% 
  rename(
    agree = top_agree
  ) %>% 
  group_by(agree) %>% 
  summarise(
    avg_roc_roc = mean(roc_roc),
    avg_prc_roc = mean(prc_roc),
    avg_roc_prc = mean(roc_prc),
    avg_prc_prc = mean(prc_prc)
  ) %>% 
  mutate(across(.cols = -agree, ~ round(., 4)))

names(test_summary)[-1] <- c("tune ROC, test ROC", "tune PRC, test ROC", 
                             "tune ROC, test PRC", "tune PRC, test PRC")

table_caption = paste0("The top row gives the average performance for trials where ROC and PRC model choice disagrees. Disagreement occurred on ", nrow(disagree_results), " out of ", nrow(test_results), " total trials. The bottom row gives performance on the trials where the two metrics agree.")

kableExtra::kbl(test_summary, caption = table_caption, escape = FALSE, align = "c") %>% 
  row_spec(0, bold=TRUE) %>% 
  column_spec(2:5, width = "2cm")


```



## Appendix

### A. Linear approximation method for resolving power

```{r signal curve derivative}
#----
# deriv_auroc
#
# Return derivative of auroc as function of ausgc
#
## arguments
# 
# ausgc   model ausgc
# v       prevalence of the positive class
#
deriv_auroc <- function(ausgc, v) {
  dydx <- -1/(2*log(v)*ausgc)
  dydx
}

```

```{r alternative resolution definition}
# now plot AUSGC as a function of the AUROC
grid_length <- 1000
prevalence <- 0.01

# evaluation coordinates
auroc_eval <- 0.8
ausgc_eval <- auroc_to_ausgc(auroc_eval, v = prevalence)

# create grid
auroc <- seq(0.5, 1, length.out=grid_length)
ausgc <- auroc_to_ausgc(auroc_quality, v = prevalence)
metric_data <- data.frame(AUROC = auroc, AUSGC = ausgc)

# create tangent line data
auroc_slope <- deriv_auroc(ausgc_eval, v = prevalence)
intercept <- auroc_eval - auroc_slope*ausgc_eval

ausgc_grid <- seq(ausgc_eval - .13, ausgc_eval + .13, length.out= 100)
auroc_grid <- intercept + auroc_slope*ausgc_grid
tangent_data <- data.frame(AUSGC = ausgc_grid, 
                           AUROC = auroc_grid)


####
# alt resolution plot
alt_plot <- ggplot(metric_data, aes(ausgc, auroc)) +
  geom_path(color="black") +
  xlab("AUSGC") +
  ylab("AUROC") +
  geom_point(mapping = aes(ausgc_eval, auroc_eval), data = plot_data, size=1.5) +
  geom_line(aes(AUSGC, AUROC), data = tangent_data, color="red") +
  theme_bw() +
  theme(
    legend.box = "horizontal",
    axis.title = element_text(size=axis_title_size),
    axis.text = element_text(size=axis_text_size),
  ) +
  NULL

```

```{r alt_resolution, fig.width=2.5, fig.height=1.5, fig.cap="\\label{fig:alt_resolution}Signal curve plotting the AUROC reference scale on the y-axis. A linear approximation of the signal function at the evaluation point is shown in red."}
alt_plot
```


### B. Binormal results for different sample sizes

```{r N1k binormal simulation data}
# read-in data
gauss_store <- read_csv("gauss_1000N_10000nsim_3nrepeats.csv")

# find average estimates across runs
gauss_results <- gauss_store %>%
  group_by(prevalence, pop_auroc) %>% 
  summarise(across(everything(), ~mean(.x)) ) %>% 
  select(-nrepeat)

# add population auprc to results
auprc_key <- auprc_grid %>% 
  unite(col = "prev_auroc", prevalence:pop_auroc, sep="_") 

gauss_results <- gauss_results %>% 
  unite(col = "prev_auroc", prevalence:pop_auroc, sep="_", remove=FALSE)

gauss_results <- left_join(gauss_results, auprc_key, "prev_auroc") %>% 
  select(-prev_auroc) %>% 
  rename(pop_auprc = auprc) %>% 
  relocate(pop_auprc, .before = pr_est)

# map auprc to auroc
prevalence <- unique(gauss_results$prevalence)
pr_mean <- NULL
pr_lower <- NULL
pr_upper <- NULL

for(v in prevalence) {
  
  grid_fun <- auprc_grid %>% 
    filter(prevalence==v)
  
  # auprc to auroc: note that it is only to 4-decimal places of precision since grid contains 1000 points
  new_mean <- auprc_to_auroc(gauss_results$pr_est[gauss_results$prevalence==v], grid_fun = grid_fun)
  new_lower <- auprc_to_auroc(gauss_results$pr_est_lower[gauss_results$prevalence==v], grid_fun = grid_fun)
  new_upper <- auprc_to_auroc(gauss_results$pr_est_upper[gauss_results$prevalence==v], grid_fun = grid_fun)
  
  pr_mean <- c(pr_mean, new_mean)
  pr_lower <- c(pr_lower, new_lower)
  pr_upper <- c(pr_upper, new_upper)
  
}

# append to main grid
gauss_results$pr_est_roc <- pr_mean
gauss_results$pr_lower_roc <- pr_lower
gauss_results$pr_upper_roc <- pr_upper

# create data for plotting
plot_data <- gauss_results %>% 
  mutate(
    auroc_bias = roc_est - pop_auroc,
    auprc_bias = pr_est - pop_auprc,
    auprc_roc_bias = pr_est_roc - pop_auroc,
    auroc_CI_width = roc_est_upper - roc_est_lower, 
    auprc_CI_width = pr_upper_roc - pr_lower_roc
  ) %>% 
  select(
    prevalence,
    pop_auroc,
    auroc_bias,
    auprc_bias,
    auprc_roc_bias,
    auroc_CI_width,
    auprc_CI_width
  ) %>% 
  mutate(
    # Use the Hosmer & Lemeshow rule of thumb: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/roc
    model_quality = case_when(
      pop_auroc==.65 ~ "Poor",
      pop_auroc==.75 ~ "Fair",
      pop_auroc==.85 ~ "Good",
      pop_auroc==.95 ~ "Excellent"
    ),
    model_quality = factor(model_quality, levels = c("Poor", "Fair", "Good", "Excellent")),
    prevalence = factor(prevalence),
    ci_ratio = auprc_CI_width/auroc_CI_width - 1
  )

  

```

```{r N1k results comparison plot}
pop_auroc <- unique(plot_data$pop_auroc)

plot_relative_1k <- ggplot(plot_data, aes(prevalence, ci_ratio, fill=prevalence)) +
  facet_wrap( ~ model_quality, nrow = 1) +
  geom_col() +
  scale_x_discrete(labels = c(".01", ".05", ".10", ".20", ".30", ".40", ".50")) +
  ylab(~ paste(kappa["AUPRC"], "/", kappa["AUROC"], " - 1")) +
  theme_bw() + 
  theme(
    axis.text = element_text(size = rr_axis_text_size),
    axis.title = element_text(size = rr_axis_title_size)
        ) +
  guides(fill="none") +
  ylim(c(-.15, .35)) +
  NULL

```


```{r N100k binormal simulation data}
# read-in data
gauss_store <- read_csv("gauss_1e+05N_10000nsim_1nrepeats.csv")

# find average estimates across runs
gauss_results <- gauss_store %>%
  group_by(prevalence, pop_auroc) %>% 
  summarise(across(everything(), ~mean(.x)) ) %>% 
  select(-nrepeat)

# add population auprc to results
auprc_key <- auprc_grid %>% 
  unite(col = "prev_auroc", prevalence:pop_auroc, sep="_") 

gauss_results <- gauss_results %>% 
  unite(col = "prev_auroc", prevalence:pop_auroc, sep="_", remove=FALSE)

gauss_results <- left_join(gauss_results, auprc_key, "prev_auroc") %>% 
  select(-prev_auroc) %>% 
  rename(pop_auprc = auprc) %>% 
  relocate(pop_auprc, .before = pr_est)

# map auprc to auroc
prevalence <- unique(gauss_results$prevalence)
pr_mean <- NULL
pr_lower <- NULL
pr_upper <- NULL

for(v in prevalence) {
  
  grid_fun <- auprc_grid %>% 
    filter(prevalence==v)
  
  # auprc to auroc: note that it is only to 4-decimal places of precision since grid contains 1000 points
  new_mean <- auprc_to_auroc(gauss_results$pr_est[gauss_results$prevalence==v], grid_fun = grid_fun)
  new_lower <- auprc_to_auroc(gauss_results$pr_est_lower[gauss_results$prevalence==v], grid_fun = grid_fun)
  new_upper <- auprc_to_auroc(gauss_results$pr_est_upper[gauss_results$prevalence==v], grid_fun = grid_fun)
  
  pr_mean <- c(pr_mean, new_mean)
  pr_lower <- c(pr_lower, new_lower)
  pr_upper <- c(pr_upper, new_upper)
  
}

# append to main grid
gauss_results$pr_est_roc <- pr_mean
gauss_results$pr_lower_roc <- pr_lower
gauss_results$pr_upper_roc <- pr_upper

# create data for plotting
plot_data <- gauss_results %>% 
  mutate(
    auroc_bias = roc_est - pop_auroc,
    auprc_bias = pr_est - pop_auprc,
    auprc_roc_bias = pr_est_roc - pop_auroc,
    auroc_CI_width = roc_est_upper - roc_est_lower, 
    auprc_CI_width = pr_upper_roc - pr_lower_roc
  ) %>% 
  select(
    prevalence,
    pop_auroc,
    auroc_bias,
    auprc_bias,
    auprc_roc_bias,
    auroc_CI_width,
    auprc_CI_width
  ) %>% 
  mutate(
    # Use the Hosmer & Lemeshow rule of thumb: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/roc
    model_quality = case_when(
      pop_auroc==.65 ~ "Poor",
      pop_auroc==.75 ~ "Fair",
      pop_auroc==.85 ~ "Good",
      pop_auroc==.95 ~ "Excellent"
    ),
    model_quality = factor(model_quality, levels = c("Poor", "Fair", "Good", "Excellent")),
    prevalence = factor(prevalence),
    ci_ratio = auprc_CI_width/auroc_CI_width - 1
  )

  

```

```{r N100k results comparison plot}
pop_auroc <- unique(plot_data$pop_auroc)

plot_relative_100k <- ggplot(plot_data, aes(prevalence, ci_ratio, fill=prevalence)) +
  facet_wrap( ~ model_quality, nrow = 1) +
  geom_col() +
  scale_x_discrete(labels = c(".01", ".05", ".10", ".20", ".30", ".40", ".50")) +
  ylab(~ paste(kappa["AUPRC"], "/", kappa["AUROC"], " - 1")) +
  theme_bw() +
  theme(
    axis.text = element_text(size = rr_axis_text_size),
    axis.title = element_text(size = rr_axis_title_size)
        ) +
  guides(fill="none") +
  ylim(c(-.15, .35)) +
  NULL

```

```{r additional_binormal, fig.width=5, fig.height=3.5, fig.cap="\\label{fig:additional_binormal}Relative metric resolution by outcome prevalence and model quality for a binormal model with a sample size of top panel: N = 1000 and bottom panel: N = 100,000. At each level of model quality 10,000 simulations are taken from the sampling model. Confidence limits are the .025 and .975 quantile values of the simulation samples."}

ggarrange(plots = list(plot_relative_1k, plot_relative_100k), nrow=2)

```


